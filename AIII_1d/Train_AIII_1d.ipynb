{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from numpy import random\n",
    "from numpy import math\n",
    "import matplotlib  \n",
    "import matplotlib.pyplot as plt \n",
    "\n",
    "# copy\n",
    "import copy\n",
    "\n",
    "\n",
    "# import Keras and TF\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "\n",
    "from tensorflow.python.keras.models import clone_model\n",
    "from tensorflow.python.keras.models import Sequential, Model\n",
    "from tensorflow.python.keras.layers import Add, Dense, Activation, Flatten, Conv2D, Conv1D, MaxPooling2D, Dropout,BatchNormalization, Input, concatenate, Lambda\n",
    "from tensorflow.python.keras.callbacks import Callback\n",
    "from tensorflow.python.keras import regularizers\n",
    "from tensorflow.python.keras.preprocessing.image import ImageDataGenerator\n",
    "\n",
    "\n",
    "from numpy import linalg as LA\n",
    "from tensorflow.python.keras import backend as K"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# download the datasets\n",
    "\n",
    "Path_trivial = 'data_AIII_expended\\\\trivial.npy'\n",
    "Path_0 = 'data_AIII_expended\\\\w_0.npy'\n",
    "Path_1 = 'data_AIII_expended\\\\w_1.npy'\n",
    "Path_random = 'data_AIII_expended\\\\w_random.npy'\n",
    "\n",
    "# data with label 0\n",
    "data_0 = np.load(Path_trivial)\n",
    "\n",
    "# data with label 1\n",
    "data_1 = np.load(Path_1)\n",
    "\n",
    "# the training dataset\n",
    "N_train = round(0.95 * data_0.shape[0])\n",
    "\n",
    "N_train_all = 2 * N_train\n",
    "\n",
    "train_data = np.zeros((N_train_all, data_0.shape[1], data_0.shape[2]), dtype = float)\n",
    "train_label = np.zeros((N_train_all), dtype = float)\n",
    "\n",
    "train_data[:N_train, :, :] = data_0[:N_train, :, :]\n",
    "train_data[N_train:, :, :] = data_1[:N_train, :, :]\n",
    "\n",
    "train_label[:N_train] = np.zeros((N_train), dtype = float)\n",
    "train_label[N_train:] = np.ones((N_train), dtype = float)\n",
    "\n",
    "\n",
    "# the test dataset\n",
    "N_test = data_0.shape[0] - round(0.95 * data_0.shape[0])\n",
    "\n",
    "N_test_all = 2 * N_test\n",
    "\n",
    "test_data = np.zeros((N_test_all, data_0.shape[1], data_0.shape[2]), dtype = float)\n",
    "test_label = np.zeros((N_test_all), dtype = float)\n",
    "\n",
    "test_data[:N_test, :, :] = data_0[N_train:, :, :]\n",
    "test_data[N_test:, :, :] = data_1[N_train:, :, :]\n",
    "\n",
    "test_label[:N_test] = np.zeros((N_test), dtype = float)\n",
    "test_label[N_test:] = np.ones((N_test), dtype = float)\n",
    "\n",
    "print(train_data.shape, train_label.shape)\n",
    "print(test_data.shape, test_label.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def setup_network() : \n",
    "    \n",
    "    # setup network \n",
    "   \n",
    "    # don't use any regularization\n",
    "    l2 = 0.000\n",
    "    \n",
    "    # setup cnn network\n",
    "    \n",
    "    input1 = Input(shape=(data_0.shape[1], data_0.shape[2]))  \n",
    " \n",
    "    # convolution layers\n",
    "    conv1_1 = Conv1D(128, kernel_size=(2), padding='valid', activation= 'relu', kernel_regularizer=regularizers.l2(l2), name='conv1_1')(input1)\n",
    " \n",
    "    conv2_1 = Conv1D(64, kernel_size=(1), padding='valid', activation= 'relu', kernel_regularizer=regularizers.l2(l2), name='conv2_1')(conv1_1)\n",
    "  \n",
    "    conv3_1 = Conv1D(32, kernel_size=(1), padding='valid', activation= 'relu', kernel_regularizer=regularizers.l2(l2), name='conv3_1')(conv2_1)\n",
    "\n",
    "    conv_all_1 = Conv1D(1, kernel_size=(1), padding='valid', activation= 'linear', kernel_regularizer=regularizers.l2(l2), name='conv_all_1')(conv3_1)\n",
    "    \n",
    "    flat = Flatten()(conv_all_1)\n",
    "    \n",
    "    # sum layer\n",
    "    dense1 = Lambda( lambda x: tf.reshape(K.sum(x, axis = 1), (-1, 1)) , name='output1')(flat)\n",
    "\n",
    "    model = Model(inputs=input1, outputs=dense1)\n",
    "\n",
    "    \n",
    "    return model\n",
    "\n",
    "\n",
    "network = setup_network()\n",
    "\n",
    "network.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# functions for doing 2d rotations: artificially expend the datasets during the training\n",
    "def matrix_rot(phi):\n",
    "\n",
    "    c = math.cos(phi)\n",
    "    s = math.sin(phi)\n",
    "    \n",
    "    R = np.array([[ c, -s], [s,  c]])\n",
    "\n",
    "    return R\n",
    "\n",
    "def rotate_random(states):\n",
    "    \n",
    "    states_final = np.zeros((states.shape[0], states.shape[1], states.shape[2]), dtype = 'float')\n",
    "    \n",
    "    for n in range(states.shape[0]):\n",
    "        \n",
    "        # rotate over a random angle 'phi' around z-axis (in (-pi, pi))\n",
    "        phi  = (2 * math.pi * random.random() - math.pi)\n",
    "        matrix = matrix_rot(phi)\n",
    "    \n",
    "        for i_x in range(states.shape[1]):\n",
    "            states_final[n, i_x, :] = np.matmul(matrix, states[n, i_x, :])\n",
    "\n",
    "            \n",
    "    return states_final\n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# specify details for the training: learning rate 0.0001\n",
    "my_adam = keras.optimizers.Adam(lr=0.00001)\n",
    "network.compile(loss='mae', optimizer=my_adam, metrics=['accuracy'])\n",
    "\n",
    "# train for 200 epoch \n",
    "for i in range(200):\n",
    "    print('Epoch: ', i)\n",
    "    \n",
    "    # rotate the states to artificially expend the training dataset - avoid overfitting (optional)\n",
    "    #train_data_rotated = rotate_random(train_data)\n",
    "                                       \n",
    "    network.fit(train_data, train_label, validation_data=(test_data, test_label), batch_size=512, epochs=1, shuffle=True)\n",
    "\n",
    "    \n",
    "# specify details for the training: learning rate 0.00001\n",
    "my_adam = keras.optimizers.Adam(lr=0.00001)\n",
    "network.compile(loss='mae', optimizer=my_adam, metrics=['accuracy'])\n",
    "\n",
    "# train for 200 epoch \n",
    "for i in range(200):\n",
    "    print('Epoch: ', i)\n",
    "    \n",
    "    # rotate the states to artificially expend the training dataset - avoid overfitting (optional)\n",
    "    #train_data_rotated = rotate_random(train_data)\n",
    "                                       \n",
    "    network.fit(train_data, train_label, validation_data=(test_data, test_label), batch_size=512, epochs=1, shuffle=True)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the network\n",
    "\n",
    "filepath = 'networks\\\\AIII_1d.h5'\n",
    "\n",
    "network.save(filepath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.python.keras.models import load_model\n",
    "\n",
    "\n",
    "filepath = 'networks\\\\AIII_1d.h5'\n",
    "\n",
    "network = load_model(filepath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "winding_num = network.predict(train_data)  \n",
    "plt.hist(winding_num, 100)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "winding_num = network.predict(test_data)  \n",
    "plt.hist(winding_num, 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a dataset of random states (preprocessed in the same way as the training and test datasets)\n",
    "def create_AIII_random(N_samples, N_k):\n",
    "    \n",
    "    data = np.zeros((N_samples, N_k + 1, 2), dtype = float)\n",
    "    \n",
    "    for n in range(N_samples):\n",
    "\n",
    "        for i_x in range(N_k) : \n",
    "            \n",
    "                \n",
    "            h_x = random.random() - 0.5\n",
    "            h_y = random.random() - 0.5\n",
    "\n",
    "            E = (h_x**2 + h_y**2)**0.5\n",
    "        \n",
    "            data[n, i_x, 0] = h_x/E \n",
    "            data[n, i_x, 1] = h_y/E \n",
    "            \n",
    "        #periodic boundary conditions\n",
    "        data[n, N_k, :] = data[n, 0, :]\n",
    "        \n",
    "    return data \n",
    "\n",
    "# interpolate from N_k + 1 to 2*N_k + 1 k-points\n",
    "def expend(data):\n",
    "    \n",
    "    N_k = data.shape[1] - 1 \n",
    "    data_new = np.zeros((data.shape[0], 2*N_k+1, 2), dtype = float)\n",
    "    \n",
    "    for n in range(data.shape[0]):    \n",
    "        \n",
    "        for i_x in range(N_k + 1):\n",
    "            data_new[n, 2*i_x, :] = data[n, i_x, :]\n",
    "    \n",
    "        for i_x in range(N_k): \n",
    "            data_new[n, 2*i_x + 1, :] = interpolate(data_new[n, 2*i_x, :], data_new[n, 2*i_x + 2, :])\n",
    "       \n",
    "  \n",
    "    return data_new\n",
    "\n",
    "\n",
    "# interpolation between two vectors          \n",
    "def interpolate(H1, H2):\n",
    "       \n",
    "    H_int = (H1 + H2)/np.linalg.norm(H1 + H2)\n",
    "    \n",
    "    return H_int\n",
    "\n",
    "\n",
    "N_samples = 1000\n",
    "N_k = 100\n",
    "data_random = expend(create_AIII_random(N_samples, N_k))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# evaluate on a dataset of random states (observe quantization of the output)\n",
    "winding_num = network.predict(data_random)  \n",
    "plt.hist(winding_num, 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
