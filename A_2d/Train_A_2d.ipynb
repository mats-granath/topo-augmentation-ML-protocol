{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from numpy import random\n",
    "from numpy import math\n",
    "import matplotlib  \n",
    "import matplotlib.pyplot as plt \n",
    "\n",
    "# copy\n",
    "import copy\n",
    "\n",
    "\n",
    "#Import Keras and TF\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "\n",
    "from tensorflow.python.keras.models import clone_model\n",
    "from tensorflow.python.keras.models import Sequential, Model\n",
    "from tensorflow.python.keras.layers import Add, Dense, Activation, Flatten, Conv2D, Conv1D, MaxPooling2D, Dropout,BatchNormalization, Input, concatenate, Lambda\n",
    "from tensorflow.python.keras.callbacks import Callback\n",
    "from tensorflow.python.keras import regularizers\n",
    "from tensorflow.python.keras.preprocessing.image import ImageDataGenerator\n",
    "\n",
    "\n",
    "from numpy import linalg as LA\n",
    "from tensorflow.python.keras import backend as K"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# download the datasets\n",
    "\n",
    "Path_trivial = 'data_A_expended\\\\trivial.npy'\n",
    "Path_0 = 'data_A_expended\\\\C_0.npy'\n",
    "Path_1 = 'data_A_expended\\\\C_1.npy'\n",
    "Path_random = 'data_A_expended\\\\C_random.npy'\n",
    "\n",
    "# data with label 0\n",
    "data_0 = np.load(Path_trivial)\n",
    "\n",
    "# data with label 1\n",
    "data_1 = np.load(Path_1)\n",
    "\n",
    "# the training dataset\n",
    "N_train = round(0.95 * data_0.shape[0])\n",
    "\n",
    "N_train_all = 2 * N_train\n",
    "\n",
    "train_data = np.zeros((N_train_all, data_0.shape[1], data_0.shape[2], data_0.shape[3]), dtype = float)\n",
    "train_label = np.zeros((N_train_all), dtype = float)\n",
    "\n",
    "train_data[:N_train, :, :, :] = data_0[:N_train, :, :, :]\n",
    "train_data[N_train:, :, :, :] = data_1[:N_train, :, :, :]\n",
    "\n",
    "train_label[:N_train] = np.zeros((N_train), dtype = float)\n",
    "train_label[N_train:] = np.ones((N_train), dtype = float)\n",
    "\n",
    "\n",
    "# the test dataset\n",
    "N_test = data_0.shape[0] - round(0.95 * data_0.shape[0])\n",
    "\n",
    "N_test_all = 2 * N_test\n",
    "\n",
    "test_data = np.zeros((N_test_all, data_0.shape[1], data_0.shape[2], data_0.shape[3]), dtype = float)\n",
    "test_label = np.zeros((N_test_all), dtype = float)\n",
    "\n",
    "test_data[:N_test, :, :, :] = data_0[N_train:, :, :, :]\n",
    "test_data[N_test:, :, :, :] = data_1[N_train:, :, :, :]\n",
    "\n",
    "test_label[:N_test] = np.zeros((N_test), dtype = float)\n",
    "test_label[N_test:] = np.ones((N_test), dtype = float)\n",
    "\n",
    "print(train_data.shape, train_label.shape)\n",
    "print(test_data.shape, test_label.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def setup_network() : \n",
    "    \n",
    "    # setup network \n",
    "    activation = 'relu'   \n",
    "\n",
    "    # don't use any regularization\n",
    "    l2 = 0.000\n",
    "    \n",
    "    # setup cnn network\n",
    "    \n",
    "    input1 = Input(shape=(data_0.shape[1], data_0.shape[2], data_0.shape[3]))  \n",
    "    \n",
    "    # convolution layers\n",
    "    conv1_1 = Conv2D(512, kernel_size=(2,2), padding='valid', activation= 'relu', kernel_regularizer=regularizers.l2(l2), name='conv1_1')(input1)\n",
    "    \n",
    "    conv2_1 = Conv2D(256, kernel_size=(1,1), padding='valid', activation= 'relu', kernel_regularizer=regularizers.l2(l2), name='conv2_1')(conv1_1)\n",
    "    \n",
    "    conv3_1 = Conv2D(128, kernel_size=(1,1), padding='valid', activation= 'relu', kernel_regularizer=regularizers.l2(l2), name='conv3_1')(conv2_1)\n",
    "    \n",
    "    conv4_1 = Conv2D(64, kernel_size=(1,1), padding='valid', activation= 'relu', kernel_regularizer=regularizers.l2(l2), name='conv4_1')(conv3_1)\n",
    "     \n",
    "    conv5_1 = Conv2D(32, kernel_size=(1,1), padding='valid', activation= 'relu', kernel_regularizer=regularizers.l2(l2), name='conv5_1')(conv4_1)\n",
    "    \n",
    "    conv_all_1 = Conv2D(1, kernel_size=(1,1), padding='valid', activation='linear', kernel_regularizer=regularizers.l2(l2), name='conv2_all_1')(conv5_1)\n",
    "    \n",
    "    flat = Flatten()(conv_all_1)\n",
    "    \n",
    "    # sum layer\n",
    "    dense1 = Lambda( lambda x: tf.reshape(K.sum(x, axis = 1), (-1, 1)) , name='output1')(flat)\n",
    "\n",
    "    model = Model(inputs=input1, outputs=dense1)\n",
    "\n",
    "    return model\n",
    "\n",
    "\n",
    "network = setup_network()\n",
    "\n",
    "network.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# functions for doing 3d rotations: artificially expend the datasets during the training\n",
    "def matrix_rot(phi, axis):\n",
    "\n",
    "    c = math.cos(phi)\n",
    "    s = math.sin(phi)\n",
    "\n",
    "\n",
    "    R1 = np.array([[ c, -s, 0], [s,  c, 0], [0,   0,  1]])\n",
    "    R2 = np.array([[ c, 0,  s], [0,  1,  0], [-s, 0,  c]])\n",
    "    R3 = np.array([[ 1, 0,   0], [0, c, -s], [0, s,  c]])\n",
    "\n",
    "    if axis == 0:\n",
    "        return R1\n",
    "    \n",
    "    if axis == 1:\n",
    "        return R2\n",
    "    \n",
    "    if axis == 2:\n",
    "        return R3\n",
    "\n",
    "def rotate_random(states):\n",
    "    \n",
    "    # rotate over random angle around x-axis (in (-pi, pi))\n",
    "    phi1  = (2 * math.pi * random.random() - math.pi)\n",
    "    matrix1 = matrix_rot(phi1, 0)\n",
    "    \n",
    "    # rotate over random angle around y-axis (in (-pi, pi))\n",
    "    phi2  = (2 * math.pi * random.random() - math.pi)\n",
    "    matrix2 = matrix_rot(phi2, 1)\n",
    "    \n",
    "    \n",
    "    # rotate over random angle around z-axis (in (-pi, pi))\n",
    "    phi3  = (2 * math.pi * random.random() - math.pi)\n",
    "    matrix3 = matrix_rot(phi3, 2)\n",
    "    \n",
    "    states_final = np.zeros((states.shape[0], states.shape[1], states.shape[2], states.shape[3]), dtype = 'float')\n",
    "    \n",
    "    for n in range(states.shape[0]):\n",
    "        for i_x in range(states.shape[1]):\n",
    "            for i_y in range(states.shape[2]):\n",
    "                states_new = np.matmul(matrix1, states[n, i_x, i_y, :])\n",
    "                states_new = np.matmul(matrix2, states_new)\n",
    "                states_final[n, i_x, i_y, :]  = np.matmul(matrix3, states_new)\n",
    "            \n",
    "    return states_final\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# specify details for the training: learning rate 0.0001\n",
    "my_adam = keras.optimizers.Adam(lr=0.0001)\n",
    "network.compile(loss='mae', optimizer=my_adam, metrics=['accuracy'])\n",
    "\n",
    "# train for 200 epoch \n",
    "for i in range(200):\n",
    "    print('Epoch: ', i)\n",
    "    \n",
    "    # rotate the states to artificially expend the training dataset - avoid overfitting (optional)\n",
    "    #train_data_rotated = rotate_random(train_data)\n",
    "                                       \n",
    "    network.fit(train_data, train_label, validation_data=(test_data, test_label), batch_size=512, epochs=1, shuffle=True)\n",
    "\n",
    "    \n",
    "# specify details for the training: learning rate 0.00001\n",
    "my_adam = keras.optimizers.Adam(lr=0.00001)\n",
    "network.compile(loss='mae', optimizer=my_adam, metrics=['accuracy'])\n",
    "\n",
    "# train for 200 epoch \n",
    "for i in range(200):\n",
    "    print('Epoch: ', i)\n",
    "    \n",
    "    # rotate the states to artificially expend the training dataset - avoid overfitting (optional)\n",
    "    #train_data_rotated = rotate_random(train_data)\n",
    "                                       \n",
    "    network.fit(train_data, train_label, validation_data=(test_data, test_label), batch_size=512, epochs=1, shuffle=True)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the network\n",
    "\n",
    "filepath = 'networks\\\\A_2d.h5'\n",
    "\n",
    "network.save(filepath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.python.keras.models import load_model\n",
    "\n",
    "\n",
    "filepath = 'networks\\\\A_2d.h5'\n",
    "\n",
    "network = load_model(filepath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chern_num = network.predict(train_data)  \n",
    "plt.hist(chern_num, 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chern_num = network.predict(test_data)  \n",
    "plt.hist(chern_num, 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a dataset of random states (preprocessed in the same way as the training and test datasets)\n",
    "def create_A_random(N_samples, N_k):\n",
    "    \n",
    "    data = np.zeros((N_samples, N_k + 1, N_k + 1, 3), dtype = float)\n",
    "    \n",
    "    for n in range(N_samples):\n",
    "\n",
    "        for i_x in range(N_k) : \n",
    "            for i_y in range(N_k) : \n",
    "            \n",
    "                \n",
    "                h_x = random.random() - 0.5\n",
    "                h_y = random.random() - 0.5\n",
    "                h_z = random.random() - 0.5\n",
    "\n",
    "                E = (h_x**2 + h_y**2 + h_z**2)**0.5\n",
    "        \n",
    "                data[n, i_x, i_y, 0] = h_x/E \n",
    "                data[n, i_x, i_y, 1] = h_y/E \n",
    "                data[n, i_x, i_y, 2] = h_z/E \n",
    "            \n",
    "        #periodic boundary conditions\n",
    "        data[n, N_k, 0, :] = data[n, 0, 0, :]\n",
    "        data[n, 0, N_k, :] = data[n, 0, 0, :]\n",
    "        data[n, N_k, N_k, :] = data[n, 0, 0, :]\n",
    "        \n",
    "        data[n, N_k, :, :] = data[n, 0, :, :]\n",
    "        data[n, :, N_k, :] = data[n, :, 0, :]\n",
    "        \n",
    "    return data \n",
    "\n",
    "# interpolate from (N_k + 1, N_k + 1)  to (2*N_k + 1, 2*N_k + 1) k-points\n",
    "def expend(data):\n",
    "    \n",
    "    N_k = np.shape(data)[1]  - 1   \n",
    "    \n",
    "    data_new = np.zeros((data.shape[0], 2*N_k + 1, 2*N_k + 1, 3), dtype = float)\n",
    "    \n",
    "    for n in range(data.shape[0]):\n",
    "        for i_x in range(N_k + 1):    \n",
    "            for i_y in range(N_k + 1): \n",
    "                data_new[n, 2*i_x, 2*i_y, :] = data[n, i_x, i_y, :]\n",
    "    \n",
    "        for i_x in range(N_k):\n",
    "            for i_y in range(N_k + 1): \n",
    "                data_new[n, 2*i_x + 1, 2*i_y, :] = interpolate(data_new[n, 2*i_x, 2*i_y, :], data_new[n, 2*i_x + 2, 2*i_y, :])\n",
    "       \n",
    "        for i_x in range(N_k + 1):\n",
    "            for i_y in range(N_k): \n",
    "                data_new[n, 2*i_x, 2*i_y + 1, :] = interpolate(data_new[n, 2*i_x, 2*i_y, :], data_new[n, 2*i_x, 2*i_y + 2, :])       \n",
    "            \n",
    "      \n",
    "        for i_x in range(N_k):\n",
    "            for i_y in range(N_k): \n",
    "                data_new[n, 2*i_x + 1, 2*i_y + 1, :] = interpolate(data_new[n, 2*i_x, 2*i_y, :], data_new[n, 2*i_x + 2, 2*i_y + 2, :])        \n",
    "            \n",
    "    return data_new \n",
    "\n",
    "\n",
    "# interpolation between two vectors      \n",
    "def interpolate(H1, H2):\n",
    "    \n",
    "    H_int = (H1 + H2)/np.linalg.norm(H1 + H2)\n",
    "        \n",
    "    return H_int\n",
    " \n",
    "\n",
    "\n",
    "N_samples = 1000\n",
    "N_k = 10\n",
    "data_random = expend(create_A_random(N_samples, N_k))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# evaluate on a dataset of random states (observe quantization of the output)\n",
    "winding_num = network.predict(data_random)  \n",
    "plt.hist(winding_num, 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
